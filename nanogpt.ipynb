{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-21 21:42:55--  https://chitanka.info/text/4618-frankenshtajn.txt.zip\n",
      "Resolving chitanka.info (chitanka.info)... 2a06:98c1:3121::2, 2a06:98c1:3120::2, 188.114.97.2, ...\n",
      "Connecting to chitanka.info (chitanka.info)|2a06:98c1:3121::2|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://m3.chitanka.info/text/4618-frankenshtajn.txt.zip?filename= [following]\n",
      "--2024-09-21 21:42:56--  https://m3.chitanka.info/text/4618-frankenshtajn.txt.zip?filename=\n",
      "Resolving m3.chitanka.info (m3.chitanka.info)... 2a06:98c1:3120::2, 2a06:98c1:3121::2, 188.114.96.2, ...\n",
      "Connecting to m3.chitanka.info (m3.chitanka.info)|2a06:98c1:3120::2|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cache/dl/Mary-Shelley_-_Frankenshtajn_-_4618.txt.zip [following]\n",
      "--2024-09-21 21:42:56--  https://m3.chitanka.info/cache/dl/Mary-Shelley_-_Frankenshtajn_-_4618.txt.zip\n",
      "Reusing existing connection to [m3.chitanka.info]:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 199044 (194K) [application/zip]\n",
      "Saving to: ‘4618-frankenshtajn.txt.zip’\n",
      "\n",
      "4618-frankenshtajn. 100%[===================>] 194.38K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-09-21 21:42:56 (6.59 MB/s) - ‘4618-frankenshtajn.txt.zip’ saved [199044/199044]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "!wget https://chitanka.info/text/4618-frankenshtajn.txt.zip\n",
    "\n",
    "path = \"4618-frankenshtajn.txt.zip\"\n",
    "\n",
    "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\".\")\n",
    "    \n",
    "!mv \"Mary-Shelley -  - . Frankenshtajn - 4618.txt\" \"dataset.txt\"\n",
    "\n",
    "!rm $path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\t\\n !$()*,-./0123456789:;=?DIMNVX[]_abcdefghijkmnoprstuvx«»АБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЮЯабвгдежзийклмнопрстуфхцчшщъьюя–—“„…\\ufeff'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = set(text)\n",
    "sorted_chars = sorted(unique_chars)\n",
    "sorted_chars\n",
    "\"\".join(sorted_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = { c: i for i, c in enumerate(sorted_chars) }\n",
    "\n",
    "def encode(text: str):\n",
    "    return [encoding[c] for c in text]\n",
    "\n",
    "def test_encode():\n",
    "    test_text = \"франкейщайн!\"\n",
    "    test_encoding = encode(test_text)\n",
    "    assert test_encoding[0] == encoding[test_text[0]]\n",
    "    assert test_encoding[1] == encoding[test_text[1]]\n",
    "    \n",
    "# test_encode()\n",
    "\n",
    "decoding = { i: c for i, c in enumerate(sorted_chars) }\n",
    "\n",
    "test_text = \"франкейщайн!\"\n",
    "test_encoding = encode(test_text)\n",
    "\n",
    "def decode(arr):\n",
    "    return \"\".join([decoding[t] for t in arr])\n",
    "\n",
    "def test_decode():\n",
    "    test_text = \"франкейщайн!\"\n",
    "    assert decode(encode(test_text)) == test_text\n",
    "    \n",
    "# test_decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120,   0,  69,  ...,  13,   1,   1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120,   0,  69,  ..., 102,  95,  99])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_size = round(len(data) * 0.9)\n",
    "\n",
    "train_data = data[:train_data_size]\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([103,  99,   2,  ...,  13,   1,   1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = data[train_data_size:]\n",
    "\n",
    "val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120,   0,  69,  90, 101,  93,   2,  81,  90])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 8\n",
    "\n",
    "train_data[:context_length+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120,   0,  69,  90, 101,  93,   2,  81])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = train_data[:context_length]\n",
    "\n",
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff\\tМери Ш'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(model_input.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(90)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output = train_data[1:context_length+1]\n",
    "\n",
    "model_output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'е'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(model_output.numpy().tolist())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 93,  87,  99, 103, 111, 103,   2,  93],\n",
       "         [ 88,  96,  90,  91,  89,  85,   8,   2],\n",
       "         [ 85,   2,  95,  85, 101,  93,  90, 101],\n",
       "         [ 95, 111, 110,  85, 103,  85,   8,   2]]),\n",
       " tensor([[ 87,  99, 103, 111, 103,   2,  93,  96],\n",
       "         [ 96,  90,  91,  89,  85,   8,   2, 102],\n",
       "         [  2,  95,  85, 101,  93,  90, 101,  85],\n",
       "         [111, 110,  85, 103,  85,   8,   2,  88]]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4 # sequences in parallel\n",
    "context_length = 8 # chars in a sequence\n",
    "\n",
    "def get_batch(split: str):\n",
    "    batch_data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(batch_data) - context_length, (batch_size,))\n",
    "    \n",
    "    x = torch.stack([batch_data[i:i+context_length] for i in ix])\n",
    "        \n",
    "    y = torch.stack([batch_data[i+1:i+context_length+1] for i in ix])\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "xb, yb # xb is input, yb is expected output (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      ".„,“M(kgе-ЮNОЕ.гнжьб$MК,Nх-6fveщс. =Ю:;vt0СpХо1$М,з—У0“6н0ШХшХкk19ж…Ж.ГмВщршн 3йN6$/гф=ц9«=еМСщ9h]…\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vocabulary_size = len(sorted_chars) # unique chars in dataset\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # returns (batch, time, channel) = (batch_size, context_length, vocabulary_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:        \n",
    "            batch, time, channel = logits.shape\n",
    "            \n",
    "            logits = logits.view(batch * time, channel)\n",
    "            targets = targets.view(batch * time)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (batch_size, context_length) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # becomes (batch_size, channel)\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1) # (batch_size, channel)\n",
    "            \n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (batch_size, 1)\n",
    "            \n",
    "            idx = torch.cat((idx, next_idx), dim=1) # (batch_size, context_length + 1)\n",
    "        return idx\n",
    "    \n",
    "model = BigramModel(vocabulary_size)\n",
    "out, loss = model(xb, yb)\n",
    "\n",
    "out.shape, loss\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "generated = model.generate(idx, max_new_tokens=100)[0].tolist()\n",
    "\n",
    "print(decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
