{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce scaling laws results from Chinchilla paper - https://arxiv.org/pdf/2203.15556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.653376"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_gpt_params(vocabulary_size, d_model, num_layers):\n",
    "    ffw_size = 4*d_model\n",
    "    attention = 3*d_model**2 + 3*d_model\n",
    "    attproj = d_model**2 + d_model\n",
    "    ffw = d_model*(ffw_size) + ffw_size\n",
    "    ffwproj = ffw_size*d_model + d_model\n",
    "    layernorms = 2*2*d_model\n",
    "    ln_f = 2*d_model\n",
    "    dense = d_model * vocabulary_size\n",
    "    # embeddings are not included in params count\n",
    "    return num_layers*(attention + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
    "\n",
    "calculate_gpt_params(vocabulary_size=50257, d_model=768, num_layers = 12) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chinchilla_params(vocabulary_size, d_model, num_layers, ffw_size):\n",
    "    attention = 3*d_model**2 + 3*d_model\n",
    "    # chinchilla uses relative position embeddings\n",
    "    relative_pos = d_model**2 + 2*d_model\n",
    "    attproj = d_model**2 + d_model\n",
    "    ffw = d_model*ffw_size + ffw_size\n",
    "    ffwproj = ffw_size*d_model + d_model\n",
    "    layernorms = 2*2*d_model\n",
    "    ln_f = 2*d_model\n",
    "    dense = d_model*vocabulary_size\n",
    "    return num_layers*(attention + relative_pos + attproj + ffw + ffwproj + layernorms) + ln_f + dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44000000.0, 512, 2048, 64, 8, 8]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# from last page of https://arxiv.org/pdf/2203.15556\n",
    "chinchilla_models_txt = '[[44000000.0, 512, 2048, 64, 8, 8], [57000000.0, 576, 2304, 64, 9, 9], [74000000.0, 640, 2560, 64, 10, 10], [90000000.0, 640, 2560, 64, 10, 13], [106000000.0, 640, 2560, 64, 10, 16], [117000000.0, 768, 3072, 64, 12, 12], [140000000.0, 768, 3072, 64, 12, 15], [163000000.0, 768, 3072, 64, 12, 18], [175000000.0, 896, 3584, 64, 14, 14], [196000000.0, 896, 3584, 64, 14, 16], [217000000.0, 896, 3584, 64, 14, 18], [251000000.0, 1024, 4096, 64, 16, 16], [278000000.0, 1024, 4096, 64, 16, 18], [306000000.0, 1024, 4096, 64, 16, 20], [425000000.0, 1280, 5120, 128, 10, 18], [489000000.0, 1280, 5120, 128, 10, 21], [509000000.0, 1408, 5632, 128, 11, 18], [552000000.0, 1280, 5120, 128, 10, 24], [587000000.0, 1408, 5632, 128, 11, 21], [632000000.0, 1536, 6144, 128, 12, 19], [664000000.0, 1408, 5632, 128, 11, 24], [724000000.0, 1536, 6144, 128, 12, 22], [816000000.0, 1536, 6144, 128, 12, 25], [893000000.0, 1792, 7168, 128, 14, 20], [1018000000.0, 1792, 7168, 128, 14, 23], [1143000000.0, 1792, 7168, 128, 14, 26], [1266000000.0, 2048, 8192, 128, 16, 22], [1424000000.0, 2176, 8704, 128, 17, 22], [1429000000.0, 2048, 8192, 128, 16, 25], [1593000000.0, 2048, 8192, 128, 16, 28], [1609000000.0, 2176, 8704, 128, 17, 25], [1731000000.0, 2304, 9216, 128, 18, 24], [1794000000.0, 2176, 8704, 128, 17, 28], [2007000000.0, 2304, 9216, 128, 18, 28], [2283000000.0, 2304, 9216, 128, 18, 32], [2298000000.0, 2560, 10240, 128, 20, 26], [2639000000.0, 2560, 10240, 128, 20, 30], [2980000000.0, 2560, 10240, 128, 20, 34], [3530000000.0, 2688, 10752, 128, 22, 36], [3802000000.0, 2816, 11264, 128, 22, 36], [4084000000.0, 2944, 11776, 128, 22, 36], [4516000000.0, 3072, 12288, 128, 24, 36], [6796000000.0, 3584, 14336, 128, 28, 40], [9293000000.0, 4096, 16384, 128, 32, 42], [11452000000.0, 4352, 17408, 128, 32, 47], [12295000000.0, 4608, 18432, 128, 36, 44], [12569000000.0, 4608, 18432, 128, 32, 47], [13735000000.0, 4864, 19456, 128, 32, 47], [14940000000.0, 4992, 19968, 128, 32, 49], [16183000000.0, 5120, 20480, 128, 40, 47]]'\n",
    "chilchilla_models = json.loads(chinchilla_models_txt) # all 50 models\n",
    "chilchilla_models[0] # tuples of params, d_model, ffw_size, kv_size, n_heads, n_layers from Table A9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated params: 12296.1623M, Chinchilla params: 12295.0000M, d_model: 4608, n_layers: 44\n",
      "Estimated params: 13124.4826M, Chinchilla params: 12569.0000M, d_model: 4608, n_layers: 47\n",
      "Estimated params: 14614.4279M, Chinchilla params: 13735.0000M, d_model: 4864, n_layers: 47\n",
      "Estimated params: 16037.5039M, Chinchilla params: 14940.0000M, d_model: 4992, n_layers: 49\n",
      "Estimated params: 16184.4582M, Chinchilla params: 16183.0000M, d_model: 5120, n_layers: 47\n"
     ]
    }
   ],
   "source": [
    "for m in chilchilla_models[-5:]: # only last 5 models from paper\n",
    "    p, d, f, k, _, l = m\n",
    "    nparams = calculate_chinchilla_params(vocabulary_size = 32000, d_model = d, num_layers = l, ffw_size=f)\n",
    "    print(f\"Estimated params: {nparams/1e6:.4f}M, Chinchilla params: {p/1e6:.4f}M, d_model: {d}, n_layers: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chinchilla_flops(context_length, d_model, num_heads, num_layers, ffw_size):\n",
    "    key_size = d_model // num_heads\n",
    "\n",
    "    attention = 2 * 3 * context_length * d_model * (key_size * num_heads)\n",
    "    attlogits = 2 * context_length * context_length * (key_size * num_heads)\n",
    "    attsoftmax = 3 * num_heads * context_length * context_length\n",
    "    attvalue = 2 * context_length * context_length * (key_size * num_heads)\n",
    "    attlinear = 2 * context_length * (key_size * num_heads) * d_model\n",
    "    att = attention + attlogits + attsoftmax + attvalue + attlinear\n",
    "    dense = 2 * context_length * (d_model * ffw_size + d_model * ffw_size)\n",
    "    \n",
    "    # do not count embeddings and logits to reproduce table 4 from paper:\n",
    "    forward_flops = num_layers * (att + dense)\n",
    "    backward_flops = 2 * forward_flops\n",
    "    return forward_flops + backward_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_length</th>\n",
       "      <th>vocabulary_size</th>\n",
       "      <th>d_model</th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>ffw_size</th>\n",
       "      <th>N</th>\n",
       "      <th>F</th>\n",
       "      <th>approximate_flops</th>\n",
       "      <th>chinchilla_flops</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2048</td>\n",
       "      <td>32000</td>\n",
       "      <td>640</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2560</td>\n",
       "      <td>73825280</td>\n",
       "      <td>929877196800</td>\n",
       "      <td>907165040640</td>\n",
       "      <td>9.298772e+11</td>\n",
       "      <td>1.025036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2048</td>\n",
       "      <td>32000</td>\n",
       "      <td>1024</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>4096</td>\n",
       "      <td>305707008</td>\n",
       "      <td>4135248199680</td>\n",
       "      <td>3756527714304</td>\n",
       "      <td>4.135248e+12</td>\n",
       "      <td>1.100817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2048</td>\n",
       "      <td>32000</td>\n",
       "      <td>1280</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>5120</td>\n",
       "      <td>552604160</td>\n",
       "      <td>7353453772800</td>\n",
       "      <td>6790399918080</td>\n",
       "      <td>7.353454e+12</td>\n",
       "      <td>1.082919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2048</td>\n",
       "      <td>32000</td>\n",
       "      <td>1792</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>7168</td>\n",
       "      <td>1143453696</td>\n",
       "      <td>14670316437504</td>\n",
       "      <td>14050759016448</td>\n",
       "      <td>1.467032e+13</td>\n",
       "      <td>1.044094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2048</td>\n",
       "      <td>32000</td>\n",
       "      <td>2048</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>8192</td>\n",
       "      <td>1593126912</td>\n",
       "      <td>20220437594112</td>\n",
       "      <td>19576343494656</td>\n",
       "      <td>2.022044e+13</td>\n",
       "      <td>1.032902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2048</td>\n",
       "      <td>32000</td>\n",
       "      <td>3584</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>14336</td>\n",
       "      <td>6796274688</td>\n",
       "      <td>83021046743040</td>\n",
       "      <td>83512623366144</td>\n",
       "      <td>8.302105e+13</td>\n",
       "      <td>0.994114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   context_length  vocabulary_size  d_model  num_heads  num_layers  ffw_size  \\\n",
       "0            2048            32000      640         10          10      2560   \n",
       "1            2048            32000     1024         16          20      4096   \n",
       "2            2048            32000     1280         10          24      5120   \n",
       "3            2048            32000     1792         14          26      7168   \n",
       "4            2048            32000     2048         16          28      8192   \n",
       "5            2048            32000     3584         28          40     14336   \n",
       "\n",
       "            N               F  approximate_flops  chinchilla_flops     ratio  \n",
       "0    73825280    929877196800       907165040640      9.298772e+11  1.025036  \n",
       "1   305707008   4135248199680      3756527714304      4.135248e+12  1.100817  \n",
       "2   552604160   7353453772800      6790399918080      7.353454e+12  1.082919  \n",
       "3  1143453696  14670316437504     14050759016448      1.467032e+13  1.044094  \n",
       "4  1593126912  20220437594112     19576343494656      2.022044e+13  1.032902  \n",
       "5  6796274688  83021046743040     83512623366144      8.302105e+13  0.994114  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# table A4 from paper appendix\n",
    "chilchilla_models_table4 = [\n",
    "  [10, 640, 2560, 10, 64],\n",
    "  [20, 1024, 4096, 16, 64],\n",
    "  [24, 1280, 5120, 10, 128 ],\n",
    "  [26, 1792, 7168, 14, 128 ],\n",
    "  [28, 2048, 8192, 16, 128],\n",
    "  [40,  3584, 14336, 28, 128]\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for num_layers, d_model, ffw_size, num_heads, _ in chilchilla_models_table4:\n",
    "    # vocabulary size is 32000 in chinchilla\n",
    "    vocabulary_size = 32000\n",
    "    context_length = 2048\n",
    "\n",
    "    D = context_length\n",
    "    N = calculate_chinchilla_params(vocabulary_size, d_model, num_layers, ffw_size)\n",
    "    F = calculate_chinchilla_flops(context_length, d_model, num_heads, num_layers, ffw_size)\n",
    "\n",
    "    approximate_flops = 6 * D * N\n",
    "    chinchilla_flops = F * (float(D) / context_length)\n",
    "    \n",
    "    rows.append({\n",
    "      'context_length': context_length,\n",
    "      'vocabulary_size': 32000,\n",
    "      'd_model': d_model,\n",
    "      'num_heads': num_heads,\n",
    "      'num_layers': num_layers,\n",
    "      'ffw_size': ffw_size,\n",
    "      'N': N,\n",
    "      'F': F,\n",
    "      'approximate_flops': approximate_flops,\n",
    "      'chinchilla_flops': chinchilla_flops,\n",
    "      'ratio': chinchilla_flops / approximate_flops,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
